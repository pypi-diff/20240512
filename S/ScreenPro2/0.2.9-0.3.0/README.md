# Comparing `tmp/ScreenPro2-0.2.9-py2.py3-none-any.whl.zip` & `tmp/ScreenPro2-0.3.0-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,20 +1,24 @@
-Zip file size: 20164 bytes, number of entries: 18
--rw-r--r--  2.0 unx     3727 b- defN 24-Apr-06 03:37 screenpro/__init__.py
--rw-r--r--  2.0 unx       22 b- defN 24-Apr-06 03:37 screenpro/__version__.py
--rw-r--r--  2.0 unx     3101 b- defN 24-Apr-06 03:37 screenpro/load.py
--rw-r--r--  2.0 unx    15758 b- defN 24-Apr-06 03:37 screenpro/phenoScore.py
--rw-r--r--  2.0 unx     1500 b- defN 24-Apr-06 03:37 screenpro/phenoStats.py
--rw-r--r--  2.0 unx     6688 b- defN 24-Apr-06 03:37 screenpro/plotting.py
--rw-r--r--  2.0 unx     3359 b- defN 24-Apr-06 03:37 screenpro/utils.py
--rw-r--r--  2.0 unx     1701 b- defN 24-Apr-06 03:37 screenpro/ngs/__init__.py
--rw-r--r--  2.0 unx     6944 b- defN 24-Apr-06 03:37 screenpro/ngs/cas12.py
--rw-r--r--  2.0 unx     3926 b- defN 24-Apr-06 03:37 screenpro/ngs/cas9.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-06 03:37 screenpro/tests/__init__.py
--rw-r--r--  2.0 unx      821 b- defN 24-Apr-06 03:37 screenpro/tests/test_fastq2count.py
--rw-r--r--  2.0 unx     1781 b- defN 24-Apr-06 03:37 screenpro/tests/test_phenoScore.py
--rw-r--r--  2.0 unx     1187 b- defN 24-Apr-06 03:41 ScreenPro2-0.2.9.dist-info/LICENSE
--rw-r--r--  2.0 unx     8042 b- defN 24-Apr-06 03:41 ScreenPro2-0.2.9.dist-info/METADATA
--rw-r--r--  2.0 unx      110 b- defN 24-Apr-06 03:41 ScreenPro2-0.2.9.dist-info/WHEEL
--rw-r--r--  2.0 unx       10 b- defN 24-Apr-06 03:41 ScreenPro2-0.2.9.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1449 b- defN 24-Apr-06 03:41 ScreenPro2-0.2.9.dist-info/RECORD
-18 files, 60126 bytes uncompressed, 17804 bytes compressed:  70.4%
+Zip file size: 29292 bytes, number of entries: 22
+-rw-r--r--  2.0 unx      292 b- defN 24-May-12 00:24 screenpro/__init__.py
+-rw-r--r--  2.0 unx       31 b- defN 24-May-12 00:24 screenpro/__main__.py
+-rw-r--r--  2.0 unx     8370 b- defN 24-May-12 00:24 screenpro/assays.py
+-rw-r--r--  2.0 unx     7406 b- defN 24-May-12 00:24 screenpro/load.py
+-rw-r--r--  2.0 unx     5343 b- defN 24-May-12 00:24 screenpro/main.py
+-rw-r--r--  2.0 unx    16261 b- defN 24-May-12 00:24 screenpro/phenoscore.py
+-rw-r--r--  2.0 unx     1694 b- defN 24-May-12 00:24 screenpro/phenostat.py
+-rw-r--r--  2.0 unx     7040 b- defN 24-May-12 00:24 screenpro/plotting.py
+-rw-r--r--  2.0 unx     3179 b- defN 24-May-12 00:24 screenpro/utils.py
+-rw-r--r--  2.0 unx     1731 b- defN 24-May-12 00:24 screenpro/ngs/__init__.py
+-rw-r--r--  2.0 unx     6944 b- defN 24-May-12 00:24 screenpro/ngs/cas12.py
+-rw-r--r--  2.0 unx     6808 b- defN 24-May-12 00:24 screenpro/ngs/cas9.py
+-rw-r--r--  2.0 unx    13859 b- defN 24-May-12 00:24 screenpro/ngs/counter.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-12 00:24 screenpro/tests/__init__.py
+-rw-r--r--  2.0 unx      821 b- defN 24-May-12 00:24 screenpro/tests/test_fastq2count.py
+-rw-r--r--  2.0 unx     1791 b- defN 24-May-12 00:24 screenpro/tests/test_phenoscore.py
+-rw-r--r--  2.0 unx     1187 b- defN 24-May-12 00:27 ScreenPro2-0.3.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx    13027 b- defN 24-May-12 00:27 ScreenPro2-0.3.0.dist-info/METADATA
+-rw-r--r--  2.0 unx      110 b- defN 24-May-12 00:27 ScreenPro2-0.3.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       50 b- defN 24-May-12 00:27 ScreenPro2-0.3.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       10 b- defN 24-May-12 00:27 ScreenPro2-0.3.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1775 b- defN 24-May-12 00:27 ScreenPro2-0.3.0.dist-info/RECORD
+22 files, 97729 bytes uncompressed, 26430 bytes compressed:  73.0%
```

## zipnote {}

```diff
@@ -1,20 +1,26 @@
 Filename: screenpro/__init__.py
 Comment: 
 
-Filename: screenpro/__version__.py
+Filename: screenpro/__main__.py
+Comment: 
+
+Filename: screenpro/assays.py
 Comment: 
 
 Filename: screenpro/load.py
 Comment: 
 
-Filename: screenpro/phenoScore.py
+Filename: screenpro/main.py
+Comment: 
+
+Filename: screenpro/phenoscore.py
 Comment: 
 
-Filename: screenpro/phenoStats.py
+Filename: screenpro/phenostat.py
 Comment: 
 
 Filename: screenpro/plotting.py
 Comment: 
 
 Filename: screenpro/utils.py
 Comment: 
@@ -24,32 +30,38 @@
 
 Filename: screenpro/ngs/cas12.py
 Comment: 
 
 Filename: screenpro/ngs/cas9.py
 Comment: 
 
+Filename: screenpro/ngs/counter.py
+Comment: 
+
 Filename: screenpro/tests/__init__.py
 Comment: 
 
 Filename: screenpro/tests/test_fastq2count.py
 Comment: 
 
-Filename: screenpro/tests/test_phenoScore.py
+Filename: screenpro/tests/test_phenoscore.py
+Comment: 
+
+Filename: ScreenPro2-0.3.0.dist-info/LICENSE
 Comment: 
 
-Filename: ScreenPro2-0.2.9.dist-info/LICENSE
+Filename: ScreenPro2-0.3.0.dist-info/METADATA
 Comment: 
 
-Filename: ScreenPro2-0.2.9.dist-info/METADATA
+Filename: ScreenPro2-0.3.0.dist-info/WHEEL
 Comment: 
 
-Filename: ScreenPro2-0.2.9.dist-info/WHEEL
+Filename: ScreenPro2-0.3.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: ScreenPro2-0.2.9.dist-info/top_level.txt
+Filename: ScreenPro2-0.3.0.dist-info/top_level.txt
 Comment: 
 
-Filename: ScreenPro2-0.2.9.dist-info/RECORD
+Filename: ScreenPro2-0.3.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## screenpro/__init__.py

```diff
@@ -1,90 +1,11 @@
-import numpy as np
-import pandas as pd
 from . import plotting as pl
-from . import phenoScore as ps
+from . import phenoscore as ps
 from . import utils
 from . import ngs
-
-from .__version__ import __version__
-from copy import copy
-
-
-class ScreenPro(object):
-    """
-    `ScreenPro` class for processing CRISPR screen datasets
-    """
-
-    def __init__(self, adata, math='log2(x+1)', test='ttest', n_reps=3):
-        """
-        Args:
-            adata (AnnData): AnnData object with adata.X as a matrix of sgRNA counts
-            math (str): math transformation to apply to the data before calculating phenotype scores
-            test (str): statistical test to use for calculating phenotype scores
-        """
-        self.adata = adata
-        self.math = math
-        self.test = test
-        self.n_reps = n_reps
-        self.phenotypes = {}
-
-    def __repr__(self):
-        descriptions = ''
-        for score_level in self.phenotypes.keys():
-            scores = "', '".join(self.phenotypes[score_level].columns.get_level_values(0).unique().to_list())
-            descriptions += f"Phenotypes in score_level = '{score_level}':\n    scores: '{scores}'\n"
-
-        return f'obs->samples\nvar->oligos\n\n{self.adata.__repr__()}\n\n{descriptions}'
-
-    def copy(self):
-        return copy(self)
-
-    def calculateDrugScreen(self, t0, untreated, treated, db_untreated, db_treated, score_level):
-        """
-        Calculate gamma, rho, and tau phenotype scores for a drug screen dataset in a given `score_level`
-        see this issue for discussion https://github.com/abearab/ScreenPro2/issues/15.
-        Args:
-            t0 (str): name of the untreated condition
-            untreated (str): name of the untreated condition
-            treated (str): name of the treated condition
-            db_untreated (float): doubling rate of the untreated condition
-            db_treated (float): doubling rate of the treated condition
-            score_level (str): name of the score level
-        """
-        # calculate phenotype scores: gamma, tau, rho
-        gamma_name, gamma = ps.runPhenoScore(
-            self.adata, cond1=t0, cond2=untreated, growth_rate=db_untreated,
-            n_reps=self.n_reps,
-            math=self.math, test=self.test, score_level=score_level
-        )
-        tau_name, tau = ps.runPhenoScore(
-            self.adata, cond1=t0, cond2=treated, growth_rate=db_treated,
-            n_reps=self.n_reps,
-            math=self.math, test=self.test, score_level=score_level
-        )
-        # TO-DO: warning / error if db_untreated and db_treated are too close, i.e. growth_rate ~= 0.
-        rho_name, rho = ps.runPhenoScore(
-            self.adata, cond1=untreated, cond2=treated, growth_rate=np.abs(db_untreated - db_treated),
-            n_reps=self.n_reps,
-            math=self.math, test=self.test, score_level=score_level
-        )
-
-        # save all results into a multi-index dataframe
-        self.phenotypes[score_level] = pd.concat({
-            f'gamma:{gamma_name}': gamma, f'tau:{tau_name}': tau, f'rho:{rho_name}': rho
-        }, axis=1)
-
-    def calculateFlowBasedScreen(self, low_bin, high_bin, score_level):
-        """
-        Calculate phenotype scores for a flow-based screen dataset
-        see this issue for discussion https://github.com/abearab/ScreenPro2/issues/17
-        """
-        # calculate phenotype scores
-        phenotype_name, phenotype = ps.runPhenoScore(
-            self.adata, cond1=low_bin, cond2=high_bin, n_reps=self.n_reps,
-            math=self.math, test=self.test, score_level=score_level
-        )
-
-        # save all results into a multi-index dataframe
-        self.phenotypes[score_level] = pd.concat({
-            f'phenotype:{phenotype_name}': phenotype
-        }, axis=1)
+from . import load
+from .ngs import Counter
+from .assays import PooledScreens, GImaps
+
+__version__ = "0.3.0"
+__author__ = "Abe Arab"
+__email__ = 'abea@arcinstitute.org' # "abarbiology@gmail.com"
```

## screenpro/load.py

```diff
@@ -1,29 +1,137 @@
 """
 Module for loading screen datasets
 """
 
-# import
 import pickle
 import pandas as pd
 
+from .utils import check_protospacer_length, trim_protospacer
+
+
+def load_cas9_sgRNA_library(library_path, library_type, sep='\t', index_col=0, protospacer_length=19, verbose=True, **args):
+    '''Load Cas9 sgRNA library table for single or dual guide design.
+    '''
+    library = pd.read_csv(
+        library_path,
+        sep=sep,
+        index_col=index_col,
+        **args
+    )
+
+    ## Evaluate library table and reformat columns for downstream analysis
+    # I would like to name the target column 'target' if it is named 'gene'!
+    
+    if library_type == "single_guide_design":
+        eval_columns = ['target', 'sgID', 'protospacer', 'sequence']
+
+        # reformating columns as needed
+        if 'gene' in library.columns:
+            # rename gene column to target
+            library = library.rename(columns={'gene': 'target'})
+        if 'sequence' in library.columns and 'protospacer' not in library.columns:
+            library.rename(columns={'sequence': 'protospacer'}, inplace=True)
+        if 'sgId' in library.columns:
+            library.rename(columns={'sgId': 'sgID'}, inplace=True)
+
+        # Upper case protospacer sequences
+        library['protospacer'] = library['protospacer'].str.upper()
+
+        protospacer_col = 'protospacer'
+        in_length = check_protospacer_length(library, 'protospacer')
+        if in_length == protospacer_length:
+            pass
+        elif in_length > protospacer_length:
+            if verbose: print(f"Trimming protospacer sequences in '{protospacer_col}' column.")
+            library = trim_protospacer(
+                library, protospacer_col, 
+                '5prime', 
+                in_length - protospacer_length
+            )
+
+        elif in_length < protospacer_length:
+            raise ValueError(
+                f"Input protospacer length for '{protospacer_col}' is less than {protospacer_length}"
+            )
+        
+        # write `sequence` column as `protospacer` (after trimming)
+        library['sequence'] = library['protospacer']
+
+        for col in eval_columns:
+            if col not in library.columns:
+                raise ValueError(f"Column '{col}' not found in library table.")
+        
+        library = library[eval_columns]
+
+    elif library_type == "dual_guide_design":
+        eval_columns = [
+            'target', 'sgID_AB',
+            'sgID_A', 'protospacer_A', 
+            'sgID_B', 'protospacer_B', 
+            'sequence'
+        ]
+        
+        # reformating columns as needed
+        if 'gene' in library.columns:
+            # rename gene column to target
+            library = library.rename(columns={'gene': 'target'})
+
+        # Upper case protospacer sequences
+        library['protospacer_A'] = library['protospacer_A'].str.upper()
+        library['protospacer_B'] = library['protospacer_B'].str.upper()
+
+        # # TODO: Enable trimming of protospacer sequences through command line arguments.
+        for protospacer_col in ['protospacer_A', 'protospacer_B']:
+            in_length = check_protospacer_length(library, protospacer_col) 
+            if in_length == protospacer_length:
+                pass
+            elif in_length > protospacer_length:
+                if verbose: print(f"Trimming protospacer sequences in '{protospacer_col}' column.")
+                library = trim_protospacer(
+                    library, protospacer_col, 
+                    '5prime', 
+                    in_length - protospacer_length
+                )
+
+            elif in_length < protospacer_length:
+                raise ValueError(
+                    f"Input protospacer length for '{protospacer_col}' is less than {protospacer_length}"
+                )
+    
+        # write `sequence` column as `protospacer_A;protospacer_B` (after trimming)
+        library['sequence'] = library['protospacer_A'] + ';' + library['protospacer_B']
+
+        for col in eval_columns:
+            if col not in library.columns:
+                raise ValueError(f"Column '{col}' not found in library table.")
+
+        library = library[eval_columns]
+
+    else:
+        raise ValueError(f"Invalid library type: {library_type}. Please choose 'single_guide_design' or 'dual_guide_design'.")
+    
+    if verbose: print("Library table successfully loaded.")
+
+    return library
+
 
-# functions
 def loadScreenProcessingData(experimentName, collapsedToTranscripts=True, premergedCounts=False):
     """
     Load ScreenProcessing outputs
     (see original code `here <https://github.com/mhorlbeck/ScreenProcessing/blob/master/screen_analysis.py#L70>`__)
     Input files:
         * `*_librarytable.txt` => library table
         * `*_mergedcountstable.txt` => merged counts table
         * `*_phenotypetable.txt` => phenotype table
-    Args:
+
+    Parameters:        
         experimentName (str): name of the experiment
         collapsedToTranscripts (bool): whether the gene scores are collapsed to transcripts
         premergedCounts (bool): whether the counts are premerged
+    
     Returns:
         dict: dictionary of dataframes
     """
     # dict of dataframes
     dataDict = {
         'library': pd.read_csv(
             experimentName + '_librarytable.txt', 
@@ -79,27 +187,29 @@
 
     return dataDict
 
 
 def write_screen_pkl(screen, name):
     """
     Write AnnData object to a pickle file
-    Args:
+    
+    Parameters:
         screen (object): ScreenPro object to save
         name (str): name of the output file (.pkl extension will be added)
     """
     file_name = f'{name}.pkl'
     with open(file_name, 'wb') as file:
         pickle.dump(screen, file)
         print(f'Object successfully saved to "{file_name}"')
 
 
 def read_screen_pkl(name):
     """
     Read ScreenPro object from a pickle file
-    Args:
+    
+    Parameters:
         name (str): name of the input file (.pkl extension will be added)
     """
     file_name = f'{name}.pkl'
     with open(file_name, 'rb') as f:
         screen = pickle.load(f)
     return screen
```

## screenpro/plotting.py

```diff
@@ -165,23 +165,32 @@
     # Annotate the points
     for i, _ in enumerate(target_data['target']):
         txt = target_data['target'].iloc[i]
         ax.annotate(txt, (target_data['score'].iloc[i] + t_x, target_data['-log10(pvalue)'].iloc[i] + t_y),
                     color='black', size=size_txt)
 
 
-def plotReplicateScatter(ax, adata, x, y, title, min_val=-2, max_val=2):
-    bdata = adata[[x, y], :].copy()
+def plotReplicateScatter(ax, adat_in, x, y, title, min_val=None, max_val=None, log_transform=True):
+    adat = adat_in[[x, y], :].copy()
 
-    bdata.obs.index = [f'Replicate {str(r)}' for r in bdata.obs.replicate.to_list()]
-    x_lab, y_lab = [f'Replicate {str(r)}' for r in bdata.obs.replicate.to_list()]
+    adat.obs.index = [f'Replicate {str(r)}' for r in adat.obs.replicate.to_list()]
+    x_lab, y_lab = [f'Replicate {str(r)}' for r in adat.obs.replicate.to_list()]
 
-    sc.pp.log1p(bdata)
+    if log_transform:
+        sc.pp.log1p(adat)
+    
+    if min_val is None:
+        min_val = min([adat.to_df().loc[x_lab,:].min(), adat.to_df().loc[y_lab,:].min()])
+        min_val = min_val * 1.1 
+    if max_val is None:
+        max_val = max([adat.to_df().loc[x_lab,:].max(), adat.to_df().loc[y_lab,:].max()])
+        max_val = max_val * 1.1
+    
     sc.pl.scatter(
-        bdata,
+        adat,
         x_lab, y_lab,
         legend_fontsize='xx-large',
         palette=[almost_black, '#BFBFBF'],
         color='targetType',
         title=title,
         size=5,
         show=False,
```

## screenpro/utils.py

```diff
@@ -1,21 +1,40 @@
 import pandas as pd
 import numpy as np
 
 
+def check_protospacer_length(library, protospacer_col):
+    lengths = list(set(library[protospacer_col].str.len()))
+    if len(lengths) > 1:
+        raise ValueError(f"Protospacer lengths are not uniform: {lengths}")
+    else:
+        length = lengths[0]
+        return length
+
+
+def trim_protospacer(library, protospacer_col, trim_side, trim_len):
+    if trim_side == '5prime':
+        library[protospacer_col] = library[protospacer_col].str[trim_len:].str.upper()
+    
+    elif trim_side == '3prime':
+        library[protospacer_col] = library[protospacer_col].str[:-trim_len].str.upper()
+    
+    return library
+
+
 def find_low_counts(adata, filter_type='either', minimum_reads=50):
     """
     Label variables with low counts in either or all samples.
 
-    Args:
-        adata: AnnData object
-        filter_type: either or all
-        minimum_reads:
+    Parameters:
+        adata (AnnData): AnnData object
+        filter_type (str): either or all
+        minimum_reads (int): minimum number of reads
 
-    Returns: it's adding a True/False column to `adata.var['low_count']`
+    Returns:
         None
     """
     count_bin = adata.X >= minimum_reads
     if filter_type == 'either':
         out = adata[:, count_bin.any(axis=0)].copy()
     elif filter_type == 'all':
         out = adata[:, count_bin.all(axis=0)].copy()
@@ -25,43 +44,26 @@
     print(
         f"{n_removed} variables with less than {minimum_reads} reads in {filter_type} replicates / experiment"
     )
 
     adata.var['low_count'] = ~adata.var.index.isin(out.var.index.to_list())
 
 
-def calculateGrowthFactor(screen, untreated, treated, db_rate_col):
-    adat = screen.adata.copy()
-
-    growth_factors = []
-
-    # calculate growth factor for gamma, tau, or rho score per replicates
-    for replicate in adat.obs.replicate.unique():
-        db_untreated = adat.obs.query(f'condition == "{untreated}" & replicate == {str(replicate)}')[db_rate_col][0]
-        db_treated = adat.obs.query(f'condition == "{treated}" & replicate == {str(replicate)}')[db_rate_col][0]
-
-        growth_factors.append(('gamma', db_untreated, replicate))
-        growth_factors.append(('tau', db_treated, replicate))
-        growth_factors.append(('rho', np.abs(db_untreated - db_treated), replicate))
-
-    out = pd.DataFrame(growth_factors, columns=['score', 'growth_factor', 'replicate'])
-
-    return out
-
-
 def ann_score_df(df_in, up_hit='resistance_hit', down_hit='sensitivity_hit', ctrl_label='non-targeting', threshold=10):
     """
     Annotate score dataframe with hit labels using given `threshold`
     (i.e. `score/pseudo_sd * -np.log10(pvalue) >= threshold`).
-    Args:
+
+    Parameters:
         df_in (pd.DataFrame): score dataframe
         up_hit (str): up hit label
         down_hit (str): down hit label
         ctrl_label (str): control label
         threshold (int): threshold
+    
     Returns:
         pd.DataFrame: annotated score dataframe
     """
     # make a copy of input dataframe
     df = df_in.copy()
 
     # rename/reformat columns
```

## screenpro/ngs/__init__.py

```diff
@@ -6,14 +6,15 @@
 ## This part of the software is conceptualized and developed by Abolfazl Arab (@abearab)
 ## with support from the Nick Youngblut (@nick-youngblut).
 
 '''Scripts to work with NGS data
 
 This module provides functions to process FASTQ files from screens with single or dual guide
 libraries. In general, the algorithm is fairly simple:
+
 1. Read the FASTQ file and extract the proper sequences
 2. Count the exact number of occurrences for each unique sequence
 3. Map the counted sequences to the reference sequence library
 4. Return the counted mapped or unmapped events as a dataframe(s)
 
 For single-guide screens, the sequences are counted as single protospacer
 from a single-end read file (R1). Then, these sequences are mapped to the reference
@@ -22,14 +23,16 @@
 For dual-guide screens, the sequences are counted as pairs of protospacer A and B
 from paired-end read files (R1 and R2). Then, sequences are mapped to the reference
 library of protospacer A and B pairs.
 
 Theoretically, the algorithm is able to detect any observed sequence since it is counting first
 and then mapping. Therefore, the recombination events can be detected. In dual-guide design
 protospacer A and B are not the same pairs as in the reference library. These events include:
+
 - Protospacer A and B pairs are present in the reference library but paired differently
 - Only one of the protospacer A and B is present in the reference library
 - None of the protospacer A and B is present in the reference library
 '''
 
-from . import cas9
 from . import cas12
+from . import cas9
+from .counter import Counter
```

## screenpro/ngs/cas9.py

```diff
@@ -12,23 +12,23 @@
     if verbose: ('count unique sequences ...')
     t0 = time()
     
     session = bb.connect()
     
     if trim5p_start and trim5p_length:
         sql_cmd = f"""
-        SELECT substr(f.sequence, {trim5p_start}, {trim5p_length}) AS sequence, COUNT(*) as count
+        SELECT substr(f.sequence, {trim5p_start}, {trim5p_length}) AS protospacer, COUNT(*) as count
         FROM fastq_scan('{fastq_file_path}') f
-        GROUP BY sequence
+        GROUP BY protospacer
         """
     else:
         sql_cmd = f"""
-        SELECT f.sequence AS sequence, COUNT(*) as count
+        SELECT f.sequence AS protospacer, COUNT(*) as count
         FROM fastq_scan('{fastq_file_path}') f
-        GROUP BY sequence
+        GROUP BY protospacer
         """
     
     df_count = session.sql(sql_cmd).to_polars()
 
     if verbose: print("done in %0.3fs" % (time() - t0))
 
     return df_count
@@ -50,15 +50,15 @@
             WITH pos1 AS (
                 SELECT REPLACE(name, '_R1', '') trimmed_name, *
                 FROM fastq_scan('{R1_fastq_file_path}')
             ), pos2 AS (
                 SELECT REPLACE(name, '_R2', '') trimmed_name, *
                 FROM fastq_scan('{R2_fastq_file_path}')
             )
-            SELECT substr(pos1.sequence, {trim5p_pos1_start}, {trim5p_pos1_length}) protospacer_A, substr(reverse_complement(pos2.sequence), {trim5p_pos2_start}, {trim5p_pos2_length}) protospacer_B, COUNT(*) count
+            SELECT substr(pos1.sequence, {trim5p_pos1_start}, {trim5p_pos1_length}) protospacer_A, reverse_complement(substr(pos2.sequence, {trim5p_pos2_start}, {trim5p_pos2_length})) protospacer_B, COUNT(*) count
             FROM pos1
             JOIN pos2
                 ON pos1.name = pos2.name
             GROUP BY protospacer_A, protospacer_B
         """
     elif trim5p_pos1_start==None and trim5p_pos1_length==None and trim5p_pos2_start==None and trim5p_pos2_length==None:
         sql_cmd = f"""
@@ -83,25 +83,115 @@
     df_count = session.sql(sql_cmd).to_polars()
 
     if verbose: print("done in %0.3fs" % (time() - t0))
 
     return df_count
 
 
-def map_sample_counts_to_library(library, sample):
-    counts_df = library.copy()
-
-    overlap = list(set(library.index.tolist()) & set(sample['sequence'].to_list()))
-    non_overlap = list(set(sample['sequence'].to_list()) - set(library.index.tolist()))
+def map_to_library_single_guide(df_count, library, return_type='all', verbose=False):
+    # get counts for given input
+    res = df_count.clone() #cheap deepcopy/clone
+    res = res.sort('count', descending=True)
+
+    res = res.with_columns(
+        pl.col("protospacer").alias("sequence"),
+    )
+
+    res_map = pl.DataFrame(library).join(
+        res, on="sequence", how="left"
+    )
+
+    if return_type == 'unmapped' or return_type == 'all':
+        res_unmap = res.join(
+            pl.DataFrame(library), on="sequence", how="anti"
+        )
+
+    if verbose:
+        print("% mapped reads",
+            100 * \
+            res_map.to_pandas()['count'].fillna(0).sum() / \
+            int(res.select(pl.sum("count")).to_pandas()['count'])
+        )
     
-    # sum counts of overlapping sequences
-    n_mapped_counts = sample.set_index('sequence').loc[overlap, 'count'].sum()
-    n_unmapped_counts = sample.set_index('sequence').loc[non_overlap, 'count'].sum()
+    if return_type == 'unmapped':
+        return res_unmap
+    elif return_type == 'mapped':
+        return res_map
+    elif return_type == 'all':
+        return {'full': res, 'mapped': res_map, 'unmapped': res_unmap}
+    else:
+        raise ValueError("return_type must be either 'unmapped', 'mapped', or 'all'")
 
-    # print number of overlapping and non-overlapping sequences
-    print(f"% mapped sequences: {n_mapped_counts/sample['count'].sum():.2f}")
-    print(f"% non-mapped sequences: {n_unmapped_counts/sample['count'].sum():.2f}")
 
-    counts_df['counts'] = 0
-    counts_df.loc[overlap, 'counts'] = sample.set_index('sequence').loc[overlap, 'count']
+def map_to_library_dual_guide(df_count, library, get_recombinant=False, return_type='all', verbose=False):
+    # get counts for given input
+    res = df_count.clone() #cheap deepcopy/clone
+    res = res.rename(
+        {'protospacer_a':'protospacer_A','protospacer_b':'protospacer_B'}
+    )
+    res = res.sort('count', descending=True)
+    res = res.with_columns(
+        pl.concat_str(
+            [
+                pl.col("protospacer_A"),
+                pl.col("protospacer_B")
+            ],
+            separator=";",
+        ).alias("sequence"),
+    )
+
+    res_map = pl.DataFrame(library).join(
+            res, on="sequence", how="left"
+        )
+
+    if get_recombinant or return_type == 'unmapped' or return_type == 'all':
+        res_unmap = res.join(
+            pl.DataFrame(library), on="sequence", how="anti"
+        )
+
+    if verbose:
+        print("% mapped reads",
+            100 * \
+            res_map.to_pandas()['count'].fillna(0).sum() / \
+            int(res.select(pl.sum("count")).to_pandas()['count'])
+        )
+    
+    if get_recombinant:
 
-    return counts_df
+        if verbose:
+            print("% unmapped reads",
+                100 * \
+                res_unmap.to_pandas()['count'].fillna(0).sum() / \
+                int(res.select(pl.sum("count")).to_pandas()['count'])
+            )
+        
+        res_unmap_remapped_a = res_unmap.join(
+            pl.DataFrame(library[['sgID_A','protospacer_A']]), on=["protospacer_A"], how="left"
+        )
+
+        res_recomb_events = res_unmap_remapped_a.join(
+            pl.DataFrame(library[['sgID_B','protospacer_B']]), 
+            on=["protospacer_B"], how="left"
+        )
+        if verbose:            
+            print("% fully remapped recombination events",
+                100 * \
+                res_recomb_events.drop_nulls().to_pandas()['count'].fillna(0).sum() / \
+                int(res.select(pl.sum("count")).to_pandas()['count'])
+            )
+    
+    if return_type == 'unmapped':
+        return res_unmap
+    elif return_type == 'mapped':
+        return res_map
+    elif return_type == 'recombinant':
+        if get_recombinant:
+            return res_recomb_events
+        else:
+            raise ValueError("get_recombinant must be set to True to calculate recombinant events")
+    elif return_type == 'all':
+        if get_recombinant:
+            return {'full': res,'mapped': res_map,'recombinant': res_recomb_events, 'unmapped': res_unmap}
+        else:
+            return {'full': res,'mapped': res_map, 'unmapped': res_unmap}
+    else:
+        raise ValueError("return_type must be either 'unmapped', 'mapped', 'recombinant', or 'all'")
```

## Comparing `screenpro/phenoScore.py` & `screenpro/phenoscore.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,130 +1,134 @@
 """
-phenoScore module
+phenoscore module
 """
 
 import numpy as np
 import pandas as pd
 from pydeseq2 import preprocessing
-from .phenoStats import matrixStat, getFDR
+from .phenostat import matrixStat, getFDR
 from .utils import find_low_counts
 
 
-def calculateDelta(x, y, math, level):
-    """
-    Calculate log ratio of y / x.
-    `level` == 'all' (i.e. averaged across all values, oligo and replicates)
+def calculateDelta(x, y, transformation, level):
+    """Calculate log ratio of y / x.
+    `level` == 'all' (i.e. averaged across all values, sgRNA library elements and replicates)
     `level` == 'col' (i.e. averaged across columns, replicates)
+
     Args:
         x (np.array): array of values
         y (np.array): array of values
-        math (str): math to use for calculating score
+        transformation (str): transformation to use for calculating score
         level (str): level to use for calculating score
+    
     Returns:
         np.array: array of log ratio values
     """
-    # check if math is implemented
-    if math not in ['log2(x+1)', 'log10', 'log1p']:
-        raise ValueError(f'math "{math}" not recognized')
+    # check if transformation is implemented
+    if transformation not in ['log2(x+1)', 'log10', 'log1p']:
+        raise ValueError(f'transformation "{transformation}" not recognized')
     
     if level == 'all':
         # average across all values
-        if math == 'log2(x+1)':
+        if transformation == 'log2(x+1)':
             return np.mean(np.log2(y+1) - np.log2(x+1))
-        elif math == 'log10':
+        elif transformation == 'log10':
             return np.mean(np.log10(y) - np.log10(x))
-        elif math == 'log1p':
+        elif transformation == 'log1p':
             return np.mean(np.log1p(y) - np.log1p(x))
     elif level == 'row':
         # average across rows
-        if math == 'log2(x+1)':
+        if transformation == 'log2(x+1)':
             return np.mean(np.log2(y+1) - np.log2(x+1), axis=0)
-        elif math == 'log10':
+        elif transformation == 'log10':
             return np.mean(np.log10(y) - np.log10(x), axis=0)
-        elif math == 'log1p':
+        elif transformation == 'log1p':
             return np.mean(np.log1p(y) - np.log1p(x), axis=0)
     elif level == 'col':
         # average across columns
-        if math == 'log2(x+1)':
+        if transformation == 'log2(x+1)':
             return np.mean(np.log2(y+1) - np.log2(x+1), axis=1)
-        elif math == 'log10':
+        elif transformation == 'log10':
             return np.mean(np.log10(y) - np.log10(x), axis=1)
-        elif math == 'log1p':
+        elif transformation == 'log1p':
             return np.mean(np.log1p(y) - np.log1p(x), axis=1)
 
 
-def calculatePhenotypeScore(x, y, x_ctrl, y_ctrl, growth_rate, math, level):
-    """
-    Calculate phenotype score normalized by negative control and growth rate.
+def calculatePhenotypeScore(x, y, x_ctrl, y_ctrl, growth_rate, transformation, level):
+    """Calculate phenotype score normalized by negative control and growth rate.
+    
     Args:
         x (np.array): array of values
         y (np.array): array of values
         x_ctrl (np.array): array of values
         y_ctrl (np.array): array of values
         growth_rate (int): growth rate
-        math (str): math to use for calculating score
+        transformation (str): transformation to use for calculating score
         level (str): level to use for calculating score
+    
     Returns:
         np.array: array of scores
     """
     # calculate control median and std
-    ctrl_median = np.median(calculateDelta(x=x_ctrl, y=y_ctrl, math=math, level=level))
+    ctrl_median = np.median(calculateDelta(x=x_ctrl, y=y_ctrl, transformation=transformation, level=level))
 
     # calculate delta
-    delta = calculateDelta(x=x, y=y, math=math, level=level)
+    delta = calculateDelta(x=x, y=y, transformation=transformation, level=level)
 
     # calculate score
     return (delta - ctrl_median) / growth_rate
 
 
-def matrixTest(x, y, x_ctrl, y_ctrl, math, level, test = 'ttest', growth_rate = 1):
-    """
-    Calculate phenotype score and p-values comparing `y` vs `x` matrices.
+def matrixTest(x, y, x_ctrl, y_ctrl, transformation, level, test = 'ttest', growth_rate = 1):
+    """Calculate phenotype score and p-values comparing `y` vs `x` matrices.
+
     Args:
         x (np.array): array of values
         y (np.array): array of values
         x_ctrl (np.array): array of values
         y_ctrl (np.array): array of values
-        math (str): math to use for calculating score
+        transformation (str): transformation to use for calculating score
         level (str): level to use for calculating score and p-value
         test (str): test to use for calculating p-value
         growth_rate (int): growth rate
+    
     Returns:
         np.array: array of scores
         np.array: array of p-values
     """
     # calculate growth score
     scores = calculatePhenotypeScore(
         x = x, y = y, x_ctrl = x_ctrl, y_ctrl = y_ctrl,
-        growth_rate = growth_rate, math = math,
+        growth_rate = growth_rate, transformation = transformation,
         level = level
     )
 
     # compute p-value
     p_values = matrixStat(x, y, test=test, level = level)
 
     return scores, p_values
 
 
-def runPhenoScore(adata, cond1, cond2, math, score_level, test,
+def runPhenoScore(adata, cond1, cond2, transformation, score_level, test,
                   growth_rate=1, n_reps=2, keep_top_n = None,num_pseudogenes=None,
                   get_z_score=False,ctrl_label='negCtrl'):
-    """
-    Calculate phenotype score and p-values when comparing `cond2` vs `cond1`.
+    """Calculate phenotype score and p-values when comparing `cond2` vs `cond1`.
+
     Args:
         adata (AnnData): AnnData object
         cond1 (str): condition 1
         cond2 (str): condition 2
-        math (str): math to use for calculating score
+        transformation (str): transformation to use for calculating score
         test (str): test to use for calculating p-value ('MW': Mann-Whitney U rank; 'ttest' : t-test)
         score_level (str): score level
         growth_rate (int): growth rate
         n_reps (int): number of replicates
         get_z_score (bool): boolean to calculate z-score normalized phenotype score and add as a new column (default is False)
         ctrl_label (str): control label
+    
     Returns:
         str: result name
         pd.DataFrame: result dataframe
     """
     # format result name
     result_name = f'{cond2}_vs_{cond1}'
     print(f'\t{cond2} vs {cond1}')
@@ -153,15 +157,15 @@
         # get control values
         x_ctrl = df_cond1[adata.var.targetType.eq(ctrl_label)].to_numpy()
         y_ctrl = df_cond2[adata.var.targetType.eq(ctrl_label)].to_numpy()
 
         # calculate growth score and p_value
         scores, p_values = matrixTest(
             x=x, y=y, x_ctrl=x_ctrl, y_ctrl=y_ctrl,
-            math=math, level='col', test=test, 
+            transformation=transformation, level='col', test=test, 
             growth_rate=growth_rate
         )
         # get adjusted p-values
         adj_p_values = getFDR(p_values)
                 
         # get targets
         targets = adata.var['target'].to_list()
@@ -224,15 +228,15 @@
                 y.sort()
                 x = x[:keep_top_n]
                 y = y[:keep_top_n]
             
             # calculate growth score and p_value
             target_scores, target_p_values = matrixTest(
                 x=x, y=y, x_ctrl=x_ctrl, y_ctrl=y_ctrl,
-                math=math, level='row', test=test,
+                transformation=transformation, level='row', test=test,
                 growth_rate=growth_rate
             )
 
             scores.append(target_scores)
             p_values.append(target_p_values)
             targets.append(target_name)
         
@@ -257,74 +261,55 @@
         raise ValueError(f'score_level "{score_level}" not recognized')
     
     
     return result_name, result
 
 
 def seqDepthNormalization(adata):
-    """
-    Normalize counts by sequencing depth.
+    """Normalize counts by sequencing depth.
+    
     Args:
         adata (AnnData): AnnData object
     """
     # normalize counts by sequencing depth
     norm_counts, size_factors = preprocessing.deseq2_norm(adata.X)
     # update adata object
     adata.obs['size_factors'] = size_factors
     adata.layers['seq_depth_norm'] = norm_counts
 
 
-def addPseudoCount():
-    pass
-    # # pseudocount
-    # if pseudocountBehavior == 'default' or pseudocountBehavior == 'zeros only':
-    #     def defaultBehavior(row):
-    #         return row if min(
-    #             row) != 0 else row + pseudocountValue
-    #
-    #     combinedCountsPseudo = combinedCounts.apply(defaultBehavior, axis=1)
-    # elif pseudocountBehavior == 'all values':
-    #     combinedCountsPseudo = combinedCounts.apply(
-    #         lambda row: row + pseudocountValue, axis=1)
-    # elif pseudocountBehavior == 'filter out':
-    #     combinedCountsPseudo = combinedCounts.copy()
-    #     zeroRows = combinedCounts.apply(lambda row: min(row) <= 0, axis=1)
-    #     combinedCountsPseudo.loc[zeroRows, :] = np.nan
-    # else:
-    #     raise ValueError(
-    #         'Pseudocount behavior not recognized or not implemented')
-
-
 def calculateZScorePhenotypeScore(score_df,ctrl_label='negCtrl'):
     """Calculate z-score normalized phenotype score.
+    
     Args:
         score_df (pd.DataFrame): dataframe of scores that includes `score` and `targetType` columns
         ctrl_label (str): control label, default is 'negCtrl'
+    
     Returns:
         pd.Series: z-score normalized phenotype score
     """
     # calculate control median and std
     ctrl_std = score_df[score_df.targetType.eq(ctrl_label)].score.std()
     # z-score normalization
     out = score_df.score / ctrl_std
 
     return out
 
 
 def runPhenoScoreForReplicate(screen, x_label, y_label, score, growth_factor_table, get_z_score=False, ctrl_label='negCtrl'):
-    """
-    Calculate phenotype score for each pair of replicates.
+    """Calculate phenotype score for each pair of replicates.
+
     Args:
         screen: ScreenPro object
         x_label: name of the first condition in column `condition` of `screen.adata.obs`
         y_label: name of the second condition in column `condition` of `screen.adata.obs`
         score: score to use for calculating phenotype score, i.e. 'gamma', 'tau', or 'rho'
         growth_factor_table: dataframe of growth factors, i.e. output from `getGrowthFactors` function
         get_z_score: boolean to calculate z-score normalized phenotype score instead of regular score (default is False)
-        ctrl_label: string to identify labels of negative control oligos
+        ctrl_label: string to identify labels of negative control elements in sgRNA library (default is 'negCtrl')
 
     Returns:
         pd.DataFrame: dataframe of phenotype scores
     """
     adat = screen.adata.copy()
 
     adat_ctrl = adat[:, adat.var.targetType.eq(ctrl_label)].copy()
@@ -336,15 +321,15 @@
             x=adat[adat.obs.query(f'condition == "{x_label}" & replicate == {str(replicate)}').index].X,
             y=adat[adat.obs.query(f'condition == "{y_label}" & replicate == {str(replicate)}').index].X,
 
             x_ctrl=adat_ctrl[adat_ctrl.obs.query(f'condition == "{x_label}" & replicate == {str(replicate)}').index].X,
             y_ctrl=adat_ctrl[adat_ctrl.obs.query(f'condition == "{y_label}" & replicate == {str(replicate)}').index].X,
 
             growth_rate=growth_factor_table.query(f'score=="{score}" & replicate=={replicate}')['growth_factor'].values[0],
-            math=screen.math,
+            transformation=screen.transformation,
             level='row'  # there is only one column so `row` option here is equivalent to the value before averaging.
         )
 
         if get_z_score:
             res = calculateZScorePhenotypeScore(
                 pd.DataFrame({'score':res,'targetType':adat.var.targetType},index=adat.var.index),
                 ctrl_label=ctrl_label
@@ -357,43 +342,65 @@
         index=adat.var.index
     )
 
     return out
 
 
 def generatePseudoGeneLabels(adata, num_pseudogenes=None, ctrl_label='negCtrl'):
-    """
-    Generate new labels per `num_pseudogenes` randomly selected non targeting oligo in `adata.var`.
+    """Generate new labels per `num_pseudogenes` randomly selected non-targeting elements in `adata.var`.
+
     Args:
         adata (AnnData): AnnData object
         num_pseudogenes (int): number of pseudogenes
         ctrl_label (str): control label
+    
+    Returns:
+        None
     """
     # check if num_pseudogenes is defined. 
-    ## If not, set to half of the number of non-targeting oligos
+    ## If not, set to half of the number of non-targeting elements 
     if num_pseudogenes is None:
         num_pseudogenes = len(adata.var[adata.var.targetType.eq(ctrl_label)]) // 2
-    # Get non-targeting oligos
-    ctrl_oligos = adata.var[adata.var.targetType.eq(ctrl_label)].index
+    # Get non-targeting elements
+    ctrl_elements = adata.var[adata.var.targetType.eq(ctrl_label)].index
     adata.var['pseudoLabel'] = ''
-    # Check if there are more than 1 non-targeting oligos to label as pseudogenes
-    if len(ctrl_oligos) / 2 <= num_pseudogenes:
-        # raise error if `num_pseudogenes` is greater than (total number of non-targeting oligos) / 2
+    # Check if there are more than 1 non-targeting elements to label as pseudogenes
+    if len(ctrl_elements) / 2 <= num_pseudogenes:
+        # raise error if `num_pseudogenes` is greater than (total number of non-targeting elements) / 2
         raise TypeError(
-            "Define `num_pseudogenes` to be less than (total number of non-targeting oligos) / 2"
+            "Define `num_pseudogenes` to be less than (total number of non-targeting elements) / 2"
         )
     else:
-        # randomly select `num` non-targeting oligos
-        while len(ctrl_oligos) > num_pseudogenes:
-            # randomly select `num` non-targeting oligos
-            pseudo_oligos = np.random.choice(ctrl_oligos, num_pseudogenes, replace=False)
+        # randomly select `num` non-targeting elements
+        while len(ctrl_elements) > num_pseudogenes:
+            # randomly select `num` non-targeting elements
+            pseudo_elements = np.random.choice(ctrl_elements, num_pseudogenes, replace=False)
             # generate new labels
             pseudo_labels = [f'pseudo_{i}' for i in range(num_pseudogenes)]
             # update adata.var
-            adata.var.loc[pseudo_oligos, 'pseudoLabel'] = pseudo_labels
-            # remove selected oligos from ctrl_oligos
-            ctrl_oligos = ctrl_oligos.drop(pseudo_oligos)
+            adata.var.loc[pseudo_elements, 'pseudoLabel'] = pseudo_labels
+            # remove selected elements from ctrl_elements
+            ctrl_elements = ctrl_elements.drop(pseudo_elements)
 
-        # label remaining non-targeting oligos as pseudogenes
+        # label remaining non-targeting elements as pseudogenes
         adata.var.loc[adata.var.targetType.eq('gene'), 'pseudoLabel'] = 'gene'
         adata.var.loc[adata.var.pseudoLabel.eq(''), 'pseudoLabel'] = np.nan
 
+# def addPseudoCount():
+    # # pseudocount
+    # if pseudocountBehavior == 'default' or pseudocountBehavior == 'zeros only':
+    #     def defaultBehavior(row):
+    #         return row if min(
+    #             row) != 0 else row + pseudocountValue
+    #
+    #     combinedCountsPseudo = combinedCounts.apply(defaultBehavior, axis=1)
+    # elif pseudocountBehavior == 'all values':
+    #     combinedCountsPseudo = combinedCounts.apply(
+    #         lambda row: row + pseudocountValue, axis=1)
+    # elif pseudocountBehavior == 'filter out':
+    #     combinedCountsPseudo = combinedCounts.copy()
+    #     zeroRows = combinedCounts.apply(lambda row: min(row) <= 0, axis=1)
+    #     combinedCountsPseudo.loc[zeroRows, :] = np.nan
+    # else:
+    #     raise ValueError(
+    #         'Pseudocount behavior not recognized or not implemented')
+
```

## Comparing `screenpro/phenoStats.py` & `screenpro/phenostat.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,24 +1,26 @@
 """
-phenoStats module
+phenostat module
 """
 
 from scipy.stats import ttest_rel
 import numpy as np
 from statsmodels.stats.multitest import multipletests
 
 
 def matrixStat(x, y, test, level):
     """
     Get p-values comparing `y` vs `x` matrices.
-    Args:
+
+    Parameters:
         x (np.array): array of values
         y (np.array): array of values
         test (str): test to use for calculating p-value
         level (str): level at which to calculate p-value
+    
     Returns:
         np.array: array of p-values
     """
     # calculate p-values
     if test == 'MW':
         # run Mann-Whitney U rank test
         raise ValueError('Mann-Whitney U rank test not implemented')
@@ -37,14 +39,21 @@
     else:
         raise ValueError(f'Test "{test}" not recognized')
 
 
 def getFDR(p_values, method='fdr_bh'):
     """
     Calculate FDR.
+
+    Parameters:
+        p_values (np.array): array of p-values
+        method (str): method to use for calculating FDR
+    
+    Returns:
+        np.array: array of adjusted p-values
     """
     # fill na with 1
     p_values[np.isnan(p_values)] = 1
     # Calculate the adjusted p-values using the Benjamini-Hochberg method
     if p_values is None:
         raise ValueError('p_values is None')
     _, adj_p_values, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')
```

## Comparing `screenpro/tests/test_phenoScore.py` & `screenpro/tests/test_phenoscore.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 import numpy as np
 import anndata as ad
 import pandas as pd
-from screenpro.phenoScore import runPhenoScore
+from screenpro.phenoscore import runPhenoScore
 
 
 def test_runPhenoScore():
     # create test data
     cond_A = np.random.randint(0, 30, size=(3, 10))
     cond_B = np.random.randint(20, 100, size=(3, 10))
 
@@ -43,15 +43,15 @@
     assert isinstance(adat, ad.AnnData)
 
     # run function
     result_name, result = runPhenoScore(
         adata=adat,
         cond1='A',
         cond2='B',
-        math='log2(x+1)',
+        transformation='log2(x+1)',
         test='ttest',
         score_level='compare_reps',
         growth_rate=1,
         n_reps=2,
         ctrl_label='negCtrl'
     )
```

## Comparing `ScreenPro2-0.2.9.dist-info/LICENSE` & `ScreenPro2-0.3.0.dist-info/LICENSE`

 * *Files identical despite different names*

