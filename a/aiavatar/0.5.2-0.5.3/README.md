# Comparing `tmp/aiavatar-0.5.2-py3-none-any.whl.zip` & `tmp/aiavatar-0.5.3-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,33 +1,33 @@
-Zip file size: 35751 bytes, number of entries: 31
+Zip file size: 37832 bytes, number of entries: 31
 -rw-r--r--  2.0 unx      712 b- defN 24-May-02 02:42 aiavatar/__init__.py
 -rw-r--r--  2.0 unx     3407 b- defN 24-May-05 01:03 aiavatar/avatar.py
--rw-r--r--  2.0 unx     7190 b- defN 24-May-05 02:35 aiavatar/bot.py
--rw-r--r--  2.0 unx     1771 b- defN 24-Apr-14 09:42 aiavatar/animation/__init__.py
--rw-r--r--  2.0 unx     1448 b- defN 24-Apr-14 09:42 aiavatar/animation/vrchat.py
+-rw-r--r--  2.0 unx     7813 b- defN 24-May-12 04:21 aiavatar/bot.py
+-rw-r--r--  2.0 unx     2196 b- defN 24-May-12 05:14 aiavatar/animation/__init__.py
+-rw-r--r--  2.0 unx     1535 b- defN 24-May-12 05:15 aiavatar/animation/vrchat.py
 -rw-r--r--  2.0 unx        0 b- defN 24-May-05 04:55 aiavatar/api/__init__.py
--rw-r--r--  2.0 unx     3976 b- defN 24-May-05 04:33 aiavatar/api/router.py
--rw-r--r--  2.0 unx     1777 b- defN 24-May-05 04:33 aiavatar/api/schema.py
+-rw-r--r--  2.0 unx     6114 b- defN 24-May-12 06:02 aiavatar/api/router.py
+-rw-r--r--  2.0 unx     2502 b- defN 24-May-12 06:06 aiavatar/api/schema.py
 -rw-r--r--  2.0 unx       31 b- defN 24-Apr-14 09:42 aiavatar/device/__init__.py
 -rw-r--r--  2.0 unx     3790 b- defN 24-Apr-20 03:57 aiavatar/device/audio.py
--rw-r--r--  2.0 unx     1650 b- defN 24-Apr-14 09:42 aiavatar/face/__init__.py
--rw-r--r--  2.0 unx     1556 b- defN 24-Apr-14 09:42 aiavatar/face/vrchat.py
--rw-r--r--  2.0 unx     6482 b- defN 24-May-05 02:35 aiavatar/listeners/__init__.py
+-rw-r--r--  2.0 unx     2052 b- defN 24-May-12 04:37 aiavatar/face/__init__.py
+-rw-r--r--  2.0 unx     1709 b- defN 24-May-12 04:39 aiavatar/face/vrchat.py
+-rw-r--r--  2.0 unx     8322 b- defN 24-May-12 04:21 aiavatar/listeners/__init__.py
 -rw-r--r--  2.0 unx     2773 b- defN 24-May-05 04:46 aiavatar/listeners/azurevoicerequest.py
 -rw-r--r--  2.0 unx     4951 b- defN 24-May-05 04:46 aiavatar/listeners/azurewakeword.py
--rw-r--r--  2.0 unx      977 b- defN 24-Apr-14 09:42 aiavatar/listeners/voicerequest.py
--rw-r--r--  2.0 unx     1133 b- defN 24-May-05 02:35 aiavatar/listeners/wakeword.py
+-rw-r--r--  2.0 unx      976 b- defN 24-May-12 04:21 aiavatar/listeners/voicerequest.py
+-rw-r--r--  2.0 unx     1132 b- defN 24-May-12 04:21 aiavatar/listeners/wakeword.py
 -rw-r--r--  2.0 unx      178 b- defN 24-Apr-14 09:42 aiavatar/processors/__init__.py
 -rw-r--r--  2.0 unx     6162 b- defN 24-May-05 11:17 aiavatar/processors/chatgpt.py
 -rw-r--r--  2.0 unx     3110 b- defN 24-May-05 11:34 aiavatar/processors/claude.py
 -rw-r--r--  2.0 unx     3388 b- defN 24-May-05 11:43 aiavatar/processors/gemini.py
 -rw-r--r--  2.0 unx     2549 b- defN 24-May-05 13:35 aiavatar/speech/__init__.py
 -rw-r--r--  2.0 unx     1887 b- defN 24-May-05 01:03 aiavatar/speech/azurespeech.py
 -rw-r--r--  2.0 unx     1201 b- defN 24-May-05 13:35 aiavatar/speech/voicevox.py
 -rw-r--r--  2.0 unx      220 b- defN 24-Apr-16 15:07 aiavatar/speech/soundplayers/__init__.py
 -rw-r--r--  2.0 unx     2145 b- defN 24-Apr-20 04:04 aiavatar/speech/soundplayers/sounddevice_player.py
--rw-r--r--  2.0 unx    11324 b- defN 24-May-05 13:37 aiavatar-0.5.2.dist-info/LICENSE
--rw-r--r--  2.0 unx    21241 b- defN 24-May-05 13:37 aiavatar-0.5.2.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-May-05 13:37 aiavatar-0.5.2.dist-info/WHEEL
--rw-r--r--  2.0 unx        9 b- defN 24-May-05 13:37 aiavatar-0.5.2.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     2604 b- defN 24-May-05 13:37 aiavatar-0.5.2.dist-info/RECORD
-31 files, 99734 bytes uncompressed, 31567 bytes compressed:  68.3%
+-rw-r--r--  2.0 unx    11324 b- defN 24-May-12 06:13 aiavatar-0.5.3.dist-info/LICENSE
+-rw-r--r--  2.0 unx    24561 b- defN 24-May-12 06:13 aiavatar-0.5.3.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-12 06:13 aiavatar-0.5.3.dist-info/WHEEL
+-rw-r--r--  2.0 unx        9 b- defN 24-May-12 06:13 aiavatar-0.5.3.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2604 b- defN 24-May-12 06:13 aiavatar-0.5.3.dist-info/RECORD
+31 files, 109445 bytes uncompressed, 33648 bytes compressed:  69.3%
```

## zipnote {}

```diff
@@ -72,23 +72,23 @@
 
 Filename: aiavatar/speech/soundplayers/__init__.py
 Comment: 
 
 Filename: aiavatar/speech/soundplayers/sounddevice_player.py
 Comment: 
 
-Filename: aiavatar-0.5.2.dist-info/LICENSE
+Filename: aiavatar-0.5.3.dist-info/LICENSE
 Comment: 
 
-Filename: aiavatar-0.5.2.dist-info/METADATA
+Filename: aiavatar-0.5.3.dist-info/METADATA
 Comment: 
 
-Filename: aiavatar-0.5.2.dist-info/WHEEL
+Filename: aiavatar-0.5.3.dist-info/WHEEL
 Comment: 
 
-Filename: aiavatar-0.5.2.dist-info/top_level.txt
+Filename: aiavatar-0.5.3.dist-info/top_level.txt
 Comment: 
 
-Filename: aiavatar-0.5.2.dist-info/RECORD
+Filename: aiavatar-0.5.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## aiavatar/bot.py

```diff
@@ -3,14 +3,15 @@
 import traceback
 # Device
 from .device import AudioDevice
 # Processor
 from .processors import ChatProcessor
 from .processors.chatgpt import ChatGPTProcessor
 # Listener
+from .listeners import NoiseLevelDetector
 from .listeners.voicerequest import VoiceRequestListener, RequestListenerBase
 from .listeners.wakeword import WakewordListener, WakewordListenerBase
 # Avatar
 from .speech import SpeechController
 from .speech.voicevox import VoicevoxSpeechController
 from .animation import AnimationController, AnimationControllerDummy
 from .face import FaceController, FaceControllerDummy
@@ -31,14 +32,17 @@
         voicevox_speaker_id: int=46,
         input_device: int=-1,
         output_device: int=-1,
         audio_devices: AudioDevice=None,
         chat_processor: ChatProcessor=None,
         request_listener: RequestListenerBase=None,
         wakeword_listener: WakewordListenerBase=None,
+        auto_noise_filter_threshold: bool=True,
+        noise_margin: float=20.0,
+        volume_threshold_db: float=-50,
         speech_controller: SpeechController=None,
         animation_controller: AnimationController=None,
         face_controller: FaceController=None,
         wakewords: list=None,
         start_voice: str=None,
         split_chars: list=None,
         language: str="ja-JP",
@@ -58,21 +62,32 @@
         else:
             self.chat_processor = ChatGPTProcessor(
                 api_key=openai_api_key,
                 model=model or "gpt-3.5-turbo",
                 system_message_content=system_message_content
             )
 
+        if auto_noise_filter_threshold:
+            noise_level_detector = NoiseLevelDetector(
+                rate=16000,
+                channels=1,
+                device_index=self.audio_devices.input_device
+            )
+            noise_level = noise_level_detector.get_noise_level()
+            volume_threshold_db = int(noise_level) + noise_margin
+
+        logger.info(f"Set volume threshold: {volume_threshold_db}dB")
+
         # Request Listener
         if request_listener:
             self.request_listener = request_listener
         else:
             self.request_listener = VoiceRequestListener(
                 google_api_key,
-                volume_threshold=2000,
+                volume_threshold=volume_threshold_db,
                 device_index=self.audio_devices.input_device,
                 lang=language
             )
         
         # Wakeword Listener
         if wakeword_listener:
             self.wakeword_listener = wakeword_listener
@@ -80,15 +95,15 @@
             async def _on_wakeword(text):
                 await self.start_chat(request_on_start=text, skip_start_voice=start_voice is None)
 
             self.wakeword_listener = WakewordListener(
                 api_key=google_api_key,
                 wakewords=wakewords or ["こんにちは" if language == "ja-JP" else "Hello"],
                 on_wakeword=_on_wakeword,
-                volume_threshold=2000,
+                volume_threshold=volume_threshold_db,
                 device_index=self.audio_devices.input_device,
                 lang=language,
                 verbose=verbose
             )
         
         self.wakeword_listener_thread = None
```

## aiavatar/animation/__init__.py

```diff
@@ -1,13 +1,18 @@
 from abc import ABC, abstractmethod
 from logging import getLogger, NullHandler
 from threading import Thread
 from time import time, sleep
 
 class AnimationController(ABC):
+    @property
+    @abstractmethod
+    def current_animation(self):
+        pass
+
     @abstractmethod
     async def animate(self, name: str, duration: float):
         pass
 
 
 class AnimationControllerBase(AnimationController):
     def __init__(self, animations: dict=None, idling_key: str="idling", verbose: bool=False):
@@ -19,19 +24,28 @@
             idling_key: 0,
             "angry_hands_on_waist": 1,
             "concern_right_hand_front": 2,
             "waving_arm": 3,
             "nodding_once": 4
         }
         self.idling_key = idling_key
+        self._current_animation = idling_key
 
         self.reset_at = None
         self.reset_thread = Thread(target=self.reset_worker, daemon=True)
         self.reset_thread.start()
 
+    @property
+    def current_animation(self) -> str:
+        return self._current_animation
+    
+    @current_animation.setter
+    def current_animation(self, name: str):
+        self._current_animation = name
+
     def reset_worker(self):
         while True:
             if self.reset_at and time() >= self.reset_at:
                 if self.verbose:
                     self.logger.info(f"Time to reset: {self.reset_at}")
                 self.reset()
                 self.reset_at = None
@@ -42,14 +56,16 @@
         self.reset_at = reset_at
         if self.verbose:
             self.logger.info(f"Reset subscribed at {self.reset_at}")
 
     async def animate(self, name: str, duration: float):
         self.subscribe_reset(time() + duration)
         self.logger.info(f"animation: {self.animations[name]} ({name})")
+        self.current_animation = name
 
     def reset(self):
         self.logger.info(f"Reset animation: {self.animations[self.idling_key]} ({self.idling_key})")
+        self.current_animation = self.idling_key
 
 
 class AnimationControllerDummy(AnimationControllerBase):
     pass
```

## aiavatar/animation/vrchat.py

```diff
@@ -19,18 +19,20 @@
         osc_value = self.animations.get(name)
         if osc_value is None:
             self.logger.warning(f"Animation '{name}' is not registered")
             return
 
         self.logger.info(f"animation: {name} ({osc_value})")
         self.client.send_message(self.osc_address, osc_value)
+        self.current_animation = name
 
     def reset(self):
         self.logger.info(f"Reset animation: {self.animations[self.idling_key]} ({self.idling_key})")
         self.client.send_message(self.osc_address, self.animations[self.idling_key])
+        self.current_animation = self.idling_key
 
     def test_osc(self):
         while True:
             self.animate(input("Animation name: "), 3.0)
 
 
 if __name__ == "__main__":
```

## aiavatar/api/router.py

```diff
@@ -45,48 +45,94 @@
     async def wakeword_status() -> WakewordStatusResponse:
         return WakewordStatusResponse(
             is_listening=aiavatr_app.is_wakeword_listener_listening(),
             thread_name=aiavatr_app.wakeword_listener_thread.name if aiavatr_app.wakeword_listener_thread else None
         )
 
 
+    @api_router.get("/avatar/status", tags=["Avatar"], name="Get avatar status")
+    async def get_avatar_status(response: Response) -> GetAvatarStatusResponse:
+        try:
+            return GetAvatarStatusResponse(
+                is_speaking=aiavatr_app.avatar_controller.speech_controller.is_speaking(),
+                current_face=aiavatr_app.avatar_controller.face_controller.current_face,
+                current_animation=aiavatr_app.avatar_controller.animation_controller.current_animation
+            )
+
+        except Exception as ex:
+            response.status_code = 500
+            return ErrorResponse(error=f"error: {ex}\n{traceback.format_exc()}")
+
+
     @api_router.post("/avatar/speech", tags=["Avatar"], name="Speak text with face expression and animation")
     async def avatar_speech(request: SpeechRequest, response: Response) -> APIResponse:
         try:
             avreq = aiavatr_app.avatar_controller.parse(request.text)
             await aiavatr_app.avatar_controller.perform(avreq)
             return APIResponse(message="success")
 
         except Exception as ex:
             response.status_code = 500
             return ErrorResponse(error=f"error: {ex}\n{traceback.format_exc()}")
 
 
+    @api_router.get("/avatar/speech/is_speaking", tags=["Avatar"], name="Check whether the avatar is speaking")
+    async def get_avatar_is_speaking(response: Response) -> GetIsSpeakingResponse:
+        try:
+            return GetIsSpeakingResponse(is_speaking=aiavatr_app.avatar_controller.speech_controller.is_speaking())
+
+        except Exception as ex:
+            response.status_code = 500
+            return ErrorResponse(error=f"error: {ex}\n{traceback.format_exc()}")
+
+
     @api_router.post("/avatar/face", tags=["Avatar"], name="Set face expression")
     async def avatar_face(request: FaceRequest, response: Response) -> APIResponse:
         try:
             await aiavatr_app.avatar_controller.face_controller.set_face(request.name, request.duration)
             return APIResponse(message="success")
 
         except Exception as ex:
             response.status_code = 500
             return ErrorResponse(error=f"error: {ex}\n{traceback.format_exc()}")
 
 
+    @api_router.get("/avatar/face", tags=["Avatar"], name="Get current face expression")
+    async def get_avatar_face(response: Response) -> GetFaceResponse:
+        try:
+            current_face = aiavatr_app.avatar_controller.face_controller.current_face
+            return GetFaceResponse(current_face=current_face)
+
+        except Exception as ex:
+            response.status_code = 500
+            return ErrorResponse(error=f"error: {ex}\n{traceback.format_exc()}")
+
+
     @api_router.post("/avatar/animation", tags=["Avatar"], name="Set animation")
     async def avatar_animation(request: AnimationRequest, response: Response) -> APIResponse:
         try:
             await aiavatr_app.avatar_controller.animation_controller.animate(request.name, request.duration)
             return APIResponse(message="success")
 
         except Exception as ex:
             response.status_code = 500
             return ErrorResponse(error=f"error: {ex}\n{traceback.format_exc()}")
 
 
+    @api_router.get("/avatar/animation", tags=["Avatar"], name="Get current animation")
+    async def get_avatar_animation(response: Response) -> GetAnimationResponse:
+        try:
+            current_animation = aiavatr_app.avatar_controller.animation_controller.current_animation
+            return GetAnimationResponse(current_animation=current_animation)
+
+        except Exception as ex:
+            response.status_code = 500
+            return ErrorResponse(error=f"error: {ex}\n{traceback.format_exc()}")
+
+
     @api_router.post("/system/log", tags=["System"], name="See the recent log")
     async def system_log(request: LogRequest, response: Response) -> LogResponse:
         try:
             with open(logfile_path, "r", encoding="utf-8") as f:
                 deque_lines = collections.deque(f, maxlen=request.count)
                 return LogResponse(lines=deque_lines)
```

## aiavatar/api/schema.py

```diff
@@ -15,27 +15,45 @@
 
 
 class WakewordStatusResponse(BaseModel):
     is_listening: bool = Field(..., description="Whether the WakewordListener is listening")
     thread_name: str|None = Field(None, example="Thread-2", description="The id of the thread wakeword listener is running in")
 
 
+class GetAvatarStatusResponse(BaseModel):
+    is_speaking: bool = Field(..., description="Whether the avatar is speaking")
+    current_face: str = Field(..., example="fun", description="Name of current face expression")
+    current_animation: str = Field(..., example="wave_hands", description="Name of current animation")
+
+
 class SpeechRequest(BaseModel):
     text: str = Field(..., example="[face:joy]Hi, let's talk with me!", description="Text to speech with face and animation tag")
 
 
+class GetIsSpeakingResponse(BaseModel):
+    is_speaking: bool = Field(..., description="Whether the avatar is speaking")
+
+
 class FaceRequest(BaseModel):
     name: str = Field(..., example="fun", description="Name of face expression to set")
     duration: float = Field(4.0, example=4.0, description="Duration in seconds for how long the face expression should last")
 
 
+class GetFaceResponse(BaseModel):
+    current_face: str = Field(..., example="fun", description="Name of current face expression")
+
+
 class AnimationRequest(BaseModel):
     name: str = Field(..., example="wave_hands", description="Name of animation to set")
     duration: float = Field(4.0, example=4.0, description="Duration in seconds for how long the animation should last")
 
 
+class GetAnimationResponse(BaseModel):
+    current_animation: str = Field(..., example="wave_hands", description="Name of current animation")
+
+
 class LogRequest(BaseModel):
     count: int = Field(50, example=50, description="Lines from tail to read")
 
 
 class LogResponse(BaseModel):
     lines: List[str] = Field(default=[], examples=["[INFO] 2024-05-05 00:25:08,070 : AzureWakeWordListener: Hello", "[INFO] 2024-05-05 00:25:13,949 : User: Hello", "[INFO] 2024-05-05 00:25:13,949 : AI: Hello! What's up?"], description="List of lines in log file")
```

## aiavatar/face/__init__.py

```diff
@@ -1,13 +1,18 @@
 from abc import ABC, abstractmethod
 from logging import getLogger, NullHandler
 from threading import Thread
 from time import time, sleep
 
 class FaceController(ABC):
+    @property
+    @abstractmethod
+    def current_face(self):
+        pass
+
     @abstractmethod
     async def set_face(self, name: str, duration: float):
         pass
 
     @abstractmethod
     def reset(self):
         pass
@@ -26,14 +31,23 @@
             "sorrow": "(; ;)",
             "fun": "(*^_^*)",
         }
 
         self.reset_at = None
         self.reset_thread = Thread(target=self.reset_worker, daemon=True)
         self.reset_thread.start()
+        self._current_face = "neutral"
+
+    @property
+    def current_face(self) -> str:
+        return self._current_face
+    
+    @current_face.setter
+    def current_face(self, name: str):
+        self._current_face = name
 
     def reset_worker(self):
         while True:
             if self.reset_at and time() >= self.reset_at:
                 if self.verbose:
                     self.logger.info(f"Time to reset: {self.reset_at}")
                 self.reset()
@@ -43,16 +57,19 @@
 
     def subscribe_reset(self, reset_at: float):
         self.reset_at = reset_at
         if self.verbose:
             self.logger.info(f"Reset subscribed at {self.reset_at}")
 
     async def set_face(self, name: str, duration: float):
-        self.subscribe_reset(time() + duration)
+        if duration > 0:
+            self.subscribe_reset(time() + duration)
         self.logger.info(f"face: {self.faces[name]} ({name})")
+        self.current_face = name
 
     def reset(self):
         self.logger.info(f"Reset face: {self.faces['neutral']} (neutral)")
+        self.current_face = "neutral"
 
 
 class FaceControllerDummy(FaceControllerBase):
     pass
```

## aiavatar/face/vrchat.py

```diff
@@ -13,33 +13,37 @@
             "joy": 1,
             "angry": 2,
             "sorrow": 3,
             "fun": 4,
             "surprise": 5,
         }
         self.neutral_key = neutral_key
+        self._current_face = self.neutral_key
         self.host = host
         self.port = port
 
         self.client = udp_client.SimpleUDPClient(self.host, self.port)
 
     async def set_face(self, name: str, duration: float):
-        self.subscribe_reset(time() + duration)
+        if duration > 0:
+            self.subscribe_reset(time() + duration)
 
         osc_value = self.faces.get(name)
         if osc_value is None:
             self.logger.warning(f"Face '{name}' is not registered")
             return
 
         self.logger.info(f"face: {name} ({osc_value})")
         self.client.send_message(self.osc_address, osc_value)
+        self.current_face = name
 
     def reset(self):
         self.logger.info(f"Reset face: {self.neutral_key} ({self.faces[self.neutral_key]})")
         self.client.send_message(self.osc_address, self.faces[self.neutral_key])
+        self.current_face = self.neutral_key
 
     def test_osc(self):
         while True:
             self.set_face(input("Face name: "), 3.0)
 
 if __name__ == "__main__":
     vrc_face_controller = VRChatFaceController()
```

## aiavatar/listeners/__init__.py

```diff
@@ -1,9 +1,10 @@
 from abc import ABC, abstractmethod
 import base64
+import collections
 from logging import getLogger, NullHandler
 import numpy
 import time
 import traceback
 from typing import Callable
 import aiohttp
 import sounddevice
@@ -21,16 +22,58 @@
         ...
 
     @abstractmethod
     def stop(self):
         ...
 
 
+class NoiseLevelDetector:
+    def __init__(self, rate: int=16000, channels: int=1, device_index: int=-1):
+        self.logger = getLogger(__name__)
+        self.logger.addHandler(NullHandler())
+
+        self.channels = channels
+        self.rate = rate
+        self.device_index = device_index
+
+    def get_volume_db(self, data: numpy.ndarray[numpy.int16], ref: int=32768) -> float:
+        amplitude = numpy.max(numpy.abs(data))
+        if amplitude == 0:
+            return -numpy.inf
+        return float(20 * numpy.log10(amplitude / ref))
+
+    def get_noise_level(self) -> float:
+        with sounddevice.InputStream(
+                device=self.device_index,
+                channels=self.channels,
+                samplerate=self.rate,
+                dtype=numpy.int16
+            ) as stream:
+
+            audio_data = collections.deque(maxlen=60)
+
+            self.logger.info("Measuring noise levels...")
+
+            while stream.active:
+                data, overflowed = stream.read(int(self.rate * 0.05))
+                if overflowed:
+                    self.logger.warning("Audio buffer has overflowed")
+
+                volume_db = self.get_volume_db(data)
+                print(f"Current: {volume_db:.2f}dB", end="\r")
+                audio_data.append(volume_db)
+            
+                if len(audio_data) == audio_data.maxlen:
+                    median_db = numpy.median(audio_data)
+                    print(f"Noise level: {median_db:.2f}dB")
+                    return median_db
+
+
 class SpeechListenerBase:
-    def __init__(self, api_key: str, on_speech_recognized: Callable, volume_threshold: int=3000, timeout: float=1.0, detection_timeout: float=0.0, min_duration: float=0.3, max_duration: float=20.0, lang: str="ja-JP", rate: int=44100, channels: int=1, device_index: int=-1):
+    def __init__(self, api_key: str, on_speech_recognized: Callable, volume_threshold: int=-50, timeout: float=1.0, detection_timeout: float=0.0, min_duration: float=0.3, max_duration: float=20.0, lang: str="ja-JP", rate: int=16000, channels: int=1, device_index: int=-1):
         self.logger = getLogger(__name__)
         self.logger.addHandler(NullHandler())
 
         self.api_key = api_key
         self.on_speech_recognized = on_speech_recognized
         self.volume_threshold = volume_threshold
         self.timeout = timeout
@@ -39,39 +82,47 @@
         self.max_duration = max_duration
         self.lang = lang
         self.channels = channels
         self.rate = rate
         self.device_index = device_index
         self.is_listening = False
 
+    def get_volume_db(self, data: numpy.ndarray[numpy.int16], ref: int=32768) -> float:
+        amplitude = numpy.max(numpy.abs(data))
+        if amplitude == 0:
+            return -numpy.inf
+        return float(20 * numpy.log10(amplitude / ref))
+
     def record_audio(self, device_index) -> bytes:
         audio_data = []
 
-        def callback(in_data, frame_count, time_info, status):
-            audio_data.append(in_data.copy())
-
         try:
             stream = sounddevice.InputStream(
                 device=device_index,
                 channels=self.channels,
                 samplerate=self.rate,
-                dtype=numpy.int16,
-                callback=callback
+                dtype=numpy.int16
             )
 
             start_time = time.time()
             is_recording = False
             silence_start_time = time.time()
             is_silent = False
             last_detected_time = time.time()
             stream.start()
 
             while stream.active:
+                data, overflowed = stream.read(int(self.rate * 0.05))   # Process frames in 0.05 sec
+                if overflowed:
+                    self.logger.warning("Audio buffer has overflowed")
+
+                audio_data.append(data)
+
                 current_time = time.time()
-                volume = numpy.linalg.norm(audio_data[-10:]) / 50 if audio_data else 0
+                volume = self.get_volume_db(data)
 
                 if not is_recording:
                     if volume > self.volume_threshold:
                         audio_data = audio_data[-100:]  # Use 100ms data before start recording
                         is_recording = True
                         start_time = current_time
```

## aiavatar/listeners/voicerequest.py

```diff
@@ -1,11 +1,11 @@
 from . import RequestListenerBase, SpeechListenerBase
 
 class VoiceRequestListener(RequestListenerBase, SpeechListenerBase):
-    def __init__(self, api_key: str, volume_threshold: int=3000, timeout: float=1.0, detection_timeout: float=10.0, min_duration: float=0.3, max_duration: float=20.0, lang: str="ja-JP", rate: int=44100, channels: int=1, device_index: int=-1):
+    def __init__(self, api_key: str, volume_threshold: int=-50, timeout: float=0.8, detection_timeout: float=10.0, min_duration: float=0.3, max_duration: float=20.0, lang: str="ja-JP", rate: int=16000, channels: int=1, device_index: int=-1):
         super().__init__(api_key, self.on_request, volume_threshold, timeout, detection_timeout, min_duration, max_duration, lang, rate, channels, device_index)
         self.last_recognized_text = None
         self.on_start_listening = None
 
     async def on_request(self, text: str):
         self.last_recognized_text = text
         self.stop_listening()
```

## aiavatar/listeners/wakeword.py

```diff
@@ -1,14 +1,14 @@
 import asyncio
 from threading import Thread
 from typing import Callable
 from . import WakewordListenerBase, SpeechListenerBase
 
 class WakewordListener(WakewordListenerBase, SpeechListenerBase):
-    def __init__(self, api_key: str, wakewords: list, on_wakeword: Callable, volume_threshold: int=3000, timeout: float=0.3, min_duration: float=0.2, max_duration: float=2, lang: str="ja-JP", rate: int=44100, chennels: int=1, device_index: int=-1, verbose: bool=False):
+    def __init__(self, api_key: str, wakewords: list, on_wakeword: Callable, volume_threshold: int=-50, timeout: float=0.2, min_duration: float=0.2, max_duration: float=2, lang: str="ja-JP", rate: int=16000, chennels: int=1, device_index: int=-1, verbose: bool=False):
         super().__init__(api_key, self.invoke_on_wakeword, volume_threshold, timeout, 0.0, min_duration, max_duration, lang, rate, chennels, device_index)
         self.wakewords = wakewords
         self.on_wakeword = on_wakeword
         self.verbose = verbose
     
     async def invoke_on_wakeword(self, text: str):
         if self.verbose:
```

## Comparing `aiavatar-0.5.2.dist-info/LICENSE` & `aiavatar-0.5.3.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `aiavatar-0.5.2.dist-info/METADATA` & `aiavatar-0.5.3.dist-info/METADATA`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: aiavatar
-Version: 0.5.2
+Version: 0.5.3
 Summary: 🥰 Building AI-based conversational avatars lightning fast ⚡️💬
 Home-page: https://github.com/uezo/aiavatar
 Author: uezo
 Author-email: uezo@uezo.net
 Maintainer: uezo
 Maintainer-email: uezo@uezo.net
 License: Apache v2
@@ -84,22 +84,24 @@
   - [🗣️ Voice](#️-voice)
   - [🐓 Wakeword Listener](#-wakeword-listener)
   - [🙏 Request Listener](#-request-listener)
   - [✨ Using Azure Listeners](#-using-azure-listeners)
   - [🔈 Audio Device](#-audio-device)
   - [🥰 Face Expression](#-face-expression)
   - [💃 Animation](#-animation)
+  - [🎭 Custom Behavior](#-custom-behavior)
 - [🌎 Platform Guide](#-platform-guide)
   - [🐈 VRChat](#-vrchat)
   - [🍓 Raspberry Pi](#-raspberry-pi)
 - [🧩 RESTful APIs](#-restful-apis)
 - [🤿 Deep Dive](#-deep-dive)
   - [⚡️ Function Calling](#️-function-calling)
 - [🔍 Other Tips](#-other-tips)
   - [🎤 Testing Audio I/O](#-testing-audio-io)
+  - [🎚️ Noise Filter](#-noise-filter)
   - [⚡️ Use Custom Listener](#️-use-custom-listener)
 
 
 # 📕 Configuration Guide
 
 Here are the configuration for each component.
 
@@ -261,15 +263,14 @@
 
 ```python
 from aiavatar.listeners.wakeword import WakewordListener
 
 wakeword_listener = WakewordListener(
     api_key=GOOGLE_API_KEY,
     wakewords=["Hello", "こんにちは"],
-    volume_threshold=2000,  # Threshold for voice detection; decrease if microphone sensitivity is low
     device_index=app.audio_devices.input_device,
     timeout=0.2,        # Duration in seconds to wait for silence before ending speech recognition
     max_duration=1.5    # Maximum duration in seconds to recognize speech before stopping
 )
 
 app.wakeword_listener = wakeword_listener
 ```
@@ -280,15 +281,14 @@
 If you want to configure in detail, create instance of `VoiceRequestListener` with custom parameters and set it to `AIAvatar`.
 
 ```python
 from aiavatar.listeners.voicerequest import VoiceRequestListener
 
 request_listener = VoiceRequestListener(
     api_key=GOOGLE_API_KEY,
-    volume_threshold=2000,  # Set lower when the microphone gain is not enough
     device_index=app.audio_devices.input_device,,
     detection_timeout=15.0, # Timeout in seconds to end the process if speech does not start within this duration
     timeout=0.5,            # Duration in seconds to wait for silence before ending speech recognition
     max_duration=20.0,      # Maximum duration in seconds to recognize speech before stopping
     min_duration=0.2,       # Minimum duration in seconds for speech to be recognized; shorter sounds are ignored
 )
 
@@ -415,48 +415,74 @@
     wakeword_listener=wakeword_listener
 )
 ```
 
 
 ## 🥰 Face expression
 
-If you want to set face expression and animation, configure as follows:
+To control facial expressions within conversations, set the facial expression names and values in `FaceController.faces` as shown below, and then include these expression keys in the response message by adding instructions to the prompt.
 
 ```python
-# Add face expresions
-app.avatar_controller.face_controller.faces["on_wake"] = 10
-app.avatar_controller.face_controller.faces["on_listening"] = 11
-app.avatar_controller.face_controller.faces["on_thinking"] = 12
+app.avatar_controller.face_controller.faces = {
+    "neutral": "🙂",
+    "joy": "😀",
+    "angry": "😠",
+    "sorrow": "😞",
+    "fun": "🥳"
+}
+
+app.chat_processor.system_message_content = """# Face Expression
+
+* You have the following expressions:
+
+- joy
+- angry
+- sorrow
+- fun
+
+* If you want to express a particular emotion, please insert it at the beginning of the sentence like [face:joy].
+
+Example
+[face:joy]Hey, you can see the ocean! [face:fun]Let's go swimming.
+"""
+```
+
+This allows emojis like 🥳 to be autonomously displayed in the terminal during conversations. To actually control the avatar's facial expressions in a metaverse platform, instead of displaying emojis like 🥳, you will need to use custom implementations tailored to the integration mechanisms of each platform. Please refer to our `VRChatFaceController` as an example.
+
+
+## 💃 Animation
+
+Now writing... ✍️
+
+
+##　🎭 Custom Behavior
+
+You can invoke custom implementations when listening to requests from user, processing those requests, or when recognized a wake word to start conversation.
 
+In the following example, changing face expressions at each timing aims to enhance the interaction experience with the AI avatar.
+
+```python
 # Set face when the character is listening the users voice
 async def set_listening_face():
-    await app.avatar_controller.face_controller.set_face("on_listening", 3.0)
+    await app.avatar_controller.face_controller.set_face("listening", 3.0)
 app.request_listener.on_start_listening = set_listening_face
 
 # Set face when the character is processing the request
 async def set_thinking_face():
-    await app.avatar_controller.face_controller.set_face("on_thinking", 3.0)
+    await app.avatar_controller.face_controller.set_face("thinking", 3.0)
 app.chat_processor.on_start_processing = set_thinking_face
 
-# Add animations (also add "walk" to the prompt)
-app.avatar_controller.animation_controller.animations["walk"] = 9
-
 async def on_wakeword(text):
     logger.info(f"Wakeword: {text}")
     # Set face when wakeword detected
-    await app.avatar_controller.face_controller.set_face("on_wake", 2.0)
+    await app.avatar_controller.face_controller.set_face("smile", 2.0)
     await app.start_chat(request_on_start=text, skip_start_voice=True)
 ```
 
 
-## 💃 Animation
-
-Now writing... ✍️
-
-
 # 🌎 Platform Guide
 
 AIAvatarKit is capable of operating on any platform that allows applications to hook into audio input and output. The platforms that have been tested include:
 
 - VRChat
 - cluster
 - Vket Cloud
@@ -464,14 +490,17 @@
 In addition to running on PCs to operate AI avatars on these platforms, you can also create a communication robot by connecting speakers, a microphone, and, if possible, a display to a Raspberry Pi.
 
 ## 🐈 VRChat
 
 * __2 Virtual audio devices (e.g. VB-CABLE) are required.__
 * __Multiple VRChat accounts are required to chat with your AIAvatar.__
 
+
+### Get started
+
 First, run the commands below in python interpreter to check the audio devices.
 
 ```bash
 $ % python
 
 >>> from aiavatar import AudioDevice
 >>> AudioDevice.list_audio_devices()
@@ -520,14 +549,86 @@
 ```
 
 Launch VRChat as desktop mode on the machine that runs `run.py` and log in with the account for AIAvatar. Then set `VB-Cable-A` to microphone in VRChat setting window.
 
 That's all! Let's chat with the AIAvatar. Log in to VRChat on another machine (or Quest) and go to the world the AIAvatar is in.
 
 
+### Face Expression
+
+AIAvatarKit controls the face expression by [Avatar OSC](https://docs.vrchat.com/docs/osc-avatar-parameters).
+
+LLM(ChatGPT/Claude/Gemini)  
+↓ *response with face tag* `[face:joy]Hello!`  
+AIAvatarKit(VRCFaceExpressionController)  
+↓ *osc* `FaceOSC=1`  
+VRChat(FX AnimatorController)  
+↓  
+😆
+
+So at first, setup your avatar the following steps:
+
+1. Add avatar parameter `FaceOSC` (type: int, default value: 0, saved: false, synced: true).
+1. Add `FaceOSC` parameter to the FX animator controller.
+1. Add layer and put states and transitions for face expression to the FX animator controller.
+1. (option) If you use the avatar that is already used in VRChat, add input parameter configuration to avatar json.
+
+
+Next, use `VRChatFaceController`.
+
+```python
+from aiavatar.face.vrchat import VRChatFaceController
+
+# Setup VRChatFaceContorller
+vrc_face_controller = VRChatFaceController(
+    faces={
+        "neutral": 0,   # always set `neutral: 0`
+
+        # key = the name that LLM can understand the expression
+        # value = FaceOSC value that is set to the transition on the FX animator controller
+        "joy": 1,
+        "angry": 2,
+        "sorrow": 3,
+        "fun": 4
+    }
+)
+```
+
+Lastly, add face expression section to the system prompt.
+
+```python
+# Make system prompt
+system_message_content = """
+# Face Expression
+
+* You have following expressions:
+
+- joy
+- angry
+- sorrow
+- fun
+
+* If you want to express a particular emotion, please insert it at the beginning of the sentence like [face:joy].
+
+Example
+[face:joy]Hey, you can see the ocean! [face:fun]Let's go swimming.
+"""
+
+# Set them to AIAvatar
+app = AIAvatar(
+    openai_api_key=OPENAI_API_KEY,
+    google_api_key=GOOGLE_API_KEY,
+    face_controller=vrc_face_controller,
+    system_message_content=system_message_content
+)
+```
+
+You can test it not only through the voice conversation but also via the [REST API](#-restful-apis).
+
+
 ## 🍓 Raspberry Pi
 
 Now writing... ✍️
 
 
 # 🧩 RESTful APIs
 
@@ -650,15 +751,14 @@
     VoicevoxSpeechController,
     WakewordListener
 )
 
 GOOGLE_API_KEY = "YOUR_API_KEY"
 VV_URL = "http://127.0.0.1:50021"
 VV_SPEAKER = 46
-VOLUME_THRESHOLD = 3000
 INPUT_DEVICE = -1
 OUTPUT_DEVICE = -1
 
 # Configure root logger
 logger = logging.getLogger()
 logger.setLevel(logging.INFO)
 log_format = logging.Formatter("[%(levelname)s] %(asctime)s : %(message)s")
@@ -696,26 +796,39 @@
 wakewords = ["こんにちは"]
 async def on_wakeword(text):
     logger.info(f"Wakeword: {text}")
     await speaker.speak(f"{text}")
 
 wakeword_listener = WakewordListener(
     api_key=GOOGLE_API_KEY,
-    volume_threshold=VOLUME_THRESHOLD,
     wakewords=["こんにちは"],
     on_wakeword=on_wakeword,
     verbose=True,
     device_index=input_device
 )
 
 # Start listening
 ww_thread = wakeword_listener.start()
 ww_thread.join()
 ```
 
+## 🎚️ Noise Filter
+
+AIAvatarKit automatically adjusts the noise filter for listeners when you instantiate an AIAvatar object. To manually set the noise filter level for voice detection, set `auto_noise_filter_threshold` to `False` and specify the `volume_threshold_db` in decibels (dB).
+
+```python
+app = AIAvatar(
+    openai_api_key=OPENAI_API_KEY,
+    google_api_key=GOOGLE_API_KEY,
+    auto_noise_filter_threshold=False,
+    volume_threshold_db=-40   # Set the voice detection threshold to -40 dB
+)
+```
+
+
 ## ⚡️ Use custom listener
 
 It's very easy to add your original listeners. Just make it run on other thread and invoke `app.start_chat()` when the listener handles the event.
 
 Here the example of `FileSystemListener` that invokes chat when `test.txt` is found on the file system.
 
 ```python
```

## Comparing `aiavatar-0.5.2.dist-info/RECORD` & `aiavatar-0.5.3.dist-info/RECORD`

 * *Files 22% similar despite different names*

```diff
@@ -1,31 +1,31 @@
 aiavatar/__init__.py,sha256=o_MsrBgCxbfFE8FeUCeDJpAtWcUmd0iopx9yPbQkUMg,712
 aiavatar/avatar.py,sha256=ML4A_-cFggDOT4Sxb7VQMHvHD_rATxhY3Pt85ZppEKg,3407
-aiavatar/bot.py,sha256=Ds48an8M5rqZf_nF5oF9tUYXoW2546Vohr2luNCzeU0,7190
-aiavatar/animation/__init__.py,sha256=aaYbs5I7vnXPEq39sJ2h7v-sG1WqfyocH4iepDSqz08,1771
-aiavatar/animation/vrchat.py,sha256=8aD3o9_NHJCiINiI-tjxF6cGevmJtvZ9IWTsixDj1Vc,1448
+aiavatar/bot.py,sha256=xVpxJixlhRcJ-9rVRLZNbeyGMoIq0y4eCLhZhhQjRT4,7813
+aiavatar/animation/__init__.py,sha256=_HEhMWleYPmPZFhqfqLVx57S0XXHDBTY_wZgNtDQLbc,2196
+aiavatar/animation/vrchat.py,sha256=KOc4zLdqR7aS69ptDVGGitdeWvKxmku_aoN31pjQSQg,1535
 aiavatar/api/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-aiavatar/api/router.py,sha256=N4SFmOqGCFc0O78X1NTK1JQNWOLTdM3syjAYlymXJYk,3976
-aiavatar/api/schema.py,sha256=r6nnnDzJO3jSEbWJw9cAlWj2nRhXQlMAdRQi2ZBjP2Q,1777
+aiavatar/api/router.py,sha256=otvzBp_BEmq-x3GgE5rmF8NX10ghOjz9LjbA_fWXS2g,6114
+aiavatar/api/schema.py,sha256=UE5Oi03LJzcivZIB_4EqLdtf47BrigOKUoMmqRABjIw,2502
 aiavatar/device/__init__.py,sha256=4DhskQxS20PcErkyF3z5-RuvXtGCQvw37jqB-yc2As8,31
 aiavatar/device/audio.py,sha256=Y_91icGtl0X6mVuder1t_PGAauBryKBhFl9goiKS6KM,3790
-aiavatar/face/__init__.py,sha256=FPaP8RT8UXQ_UMON7RLvg2FUotOzTFVJ1qxzVk5zBTw,1650
-aiavatar/face/vrchat.py,sha256=VOibFm5Yuof-RQGfd1Zt1tx4v-TV9Usb8aMn7rHp5HY,1556
-aiavatar/listeners/__init__.py,sha256=aTwTPr0lIejejPK62grL25ei_ulsIKGf8j78dYKBWqU,6482
+aiavatar/face/__init__.py,sha256=gwrBXrUGbYPofSCUOfWU-dyXDJ20TVkXkRFLkwZWTs0,2052
+aiavatar/face/vrchat.py,sha256=lG_S7QEG-tK7yHpT4qk1sGKZwD7tiRYDfs8AsMtGkoI,1709
+aiavatar/listeners/__init__.py,sha256=uCNw86Q1ctwDB9vX-ypZ1CnSOLpss2DIb0PMITAk5jg,8322
 aiavatar/listeners/azurevoicerequest.py,sha256=3DHxZl9uPLEvVH2c1gpbBmwUcVFlPaOzyO35JrIXyoc,2773
 aiavatar/listeners/azurewakeword.py,sha256=2GP_bUi56oFjARXGKFHES_KztA_MBNdiXkbYkor1eCQ,4951
-aiavatar/listeners/voicerequest.py,sha256=cvq9IN_VA0idRzC4UFwkQZ-92IYjll94-ZaijVlLTco,977
-aiavatar/listeners/wakeword.py,sha256=PeKLBiGOYHr_bL6Pdugu8zvga0AMxZROpDIezV6yGog,1133
+aiavatar/listeners/voicerequest.py,sha256=6h86YcMXUX_yFd5AAMo4zeQ80xhPTUwhSCQzgWd4ZuY,976
+aiavatar/listeners/wakeword.py,sha256=ZEGGKn5gmNZ8woSUKEbgLJX7o5iR3JD1rG3rRhoehoY,1132
 aiavatar/processors/__init__.py,sha256=k6qF1_UopWpaxQ89OxP7-dSVLgnrDCuuZH0Gom0JLLU,178
 aiavatar/processors/chatgpt.py,sha256=CyMpH_Nq-HtM50eFMFmOF7cJezk5x6VOHq9OxeTYFBU,6162
 aiavatar/processors/claude.py,sha256=c56BtK4Qsi-I4tQw0J_Jvflgu9bNXpCVV-EMqZEMhV0,3110
 aiavatar/processors/gemini.py,sha256=9cRMEpxFHsoxABysoPch-CPMUkRdD0-gO5AaqxRsuYk,3388
 aiavatar/speech/__init__.py,sha256=dwDsV_gvum9MYQI0YeSUkkPSH-Wl-EN-dVE9Isy9Am0,2549
 aiavatar/speech/azurespeech.py,sha256=P8lnUMj4j8BRnasxCxCb-xkkFA13BvHUG03aZ04RXNU,1887
 aiavatar/speech/voicevox.py,sha256=ptPd9z8ht1V1RLABpCn0V6WqC2XP8Xq6W4n2VkxFxFI,1201
 aiavatar/speech/soundplayers/__init__.py,sha256=71xYi-8qaInz_Um8wV5zDaHpAB5sTJG9osn17aQAUKQ,220
 aiavatar/speech/soundplayers/sounddevice_player.py,sha256=tBHMJ5eOiM_rKkvCWlBPE0254Ro4lcNkq8XWLB2tHP8,2145
-aiavatar-0.5.2.dist-info/LICENSE,sha256=UOZ1F5fFDe3XXvG4oNnkL1-Ecun7zpHzRxjp-XsMeAo,11324
-aiavatar-0.5.2.dist-info/METADATA,sha256=69YXKViBuhdjFXsf3thMkMBlFRWIhYtSTtYamo-Yf8I,21241
-aiavatar-0.5.2.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-aiavatar-0.5.2.dist-info/top_level.txt,sha256=B14WVaakM_kKuOkCeI-WRP83BJ6APkOyQrn6EXxs8kg,9
-aiavatar-0.5.2.dist-info/RECORD,,
+aiavatar-0.5.3.dist-info/LICENSE,sha256=UOZ1F5fFDe3XXvG4oNnkL1-Ecun7zpHzRxjp-XsMeAo,11324
+aiavatar-0.5.3.dist-info/METADATA,sha256=XbhDR8M-jHnLe9lMUqh1wEZSRIuJIxH4eKHubKwKSKw,24561
+aiavatar-0.5.3.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+aiavatar-0.5.3.dist-info/top_level.txt,sha256=B14WVaakM_kKuOkCeI-WRP83BJ6APkOyQrn6EXxs8kg,9
+aiavatar-0.5.3.dist-info/RECORD,,
```

